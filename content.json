{"meta":{"title":"xulei's BLog","subtitle":"一个全栈工程师已经不能填满我的梦想","description":null,"author":"徐磊","url":"http://Metatronxl.github.io"},"pages":[],"posts":[{"title":"python 爬虫总结帖（一）","slug":"python-爬虫总结帖（一）","date":"2017-12-05T08:47:58.000Z","updated":"2017-12-05T08:48:35.000Z","comments":true,"path":"2017/12/05/python-爬虫总结帖（一）/","link":"","permalink":"http://Metatronxl.github.io/2017/12/05/python-爬虫总结帖（一）/","excerpt":"","text":"主要是re，requests，BeautifulSoup，urllib2的使用总结 1.requests基本用法示例 使用requests加载网站 12345678910111213141516171819202122232425# coding:utf-8import requests# 下载新浪新闻首页的内容url = &apos;http://news.sina.com.cn/china/&apos;# 用get函数发送GET请求，获取响应res = requests.get(url)# 设置响应的编码格式utf-8（默认格式为ISO-8859-1），防止中文出现乱码res.encoding = &apos;utf-8&apos;print type(res)print resprint res.text# 输出：&apos;&apos;&apos;&lt;class &apos;requests.models.Response&apos;&gt;&lt;Response [200]&gt;&lt;!DOCTYPE html&gt;&lt;!-- [ published at 2017-04-19 23:30:28 ] --&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt;&lt;title&gt;国内新闻_新闻中心_新浪网&lt;/title&gt;&lt;meta name=&quot;keywords&quot; content=&quot;国内时政,内地新闻&quot;&gt; requests 发送请求与传递参数 1234567import requests r = requests.get(url=&apos;http://www.itwhy.org&apos;) # 最基本的GET请求print(r.status_code) # 获取返回状态r = requests.get(url=&apos;http://dict.baidu.com/s&apos;, params=&#123;&apos;wd&apos;:&apos;python&apos;&#125;) #带参数的GET请求print(r.url)print(r.text) #打印解码后的返回数据 requests 接口统一，使用方便 requests.get(‘https://github.com/timeline.json’) #GET请求 requests.post(“http://httpbin.org/post”) #POST请求 requests.put(“http://httpbin.org/put”) #PUT请求 requests.delete(“http://httpbin.org/delete”) #DELETE请求 requests.head(“http://httpbin.org/get”) #HEAD请求 requests.options(“http://httpbin.org/get”) #OPTIONS请求 ### requests使用post发送json请求 12345import requestsimport json r = requests.post(&apos;https://api.github.com/some/endpoint&apos;, data=json.dumps(&#123;&apos;some&apos;: &apos;data&apos;&#125;))print(r.json()) requests 定制header： 12345678910import requestsimport json data = &#123;&apos;some&apos;: &apos;data&apos;&#125;headers = &#123;&apos;content-type&apos;: &apos;application/json&apos;, &apos;User-Agent&apos;: &apos;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:22.0) Gecko/20100101 Firefox/22.0&apos;&#125; r = requests.post(&apos;https://api.github.com/some/endpoint&apos;, data=data, headers=headers)print(r.text requests 代理访问 有时候为了避免封IP，或者在某些公司内网访问外网时候，需要用到代理服务器发送请求，代理的用法示例： 1234import requestsproxies = &#123;&apos;http&apos;:&apos;http://proxy.test.com:8080&apos;,&apos;https&apos;:&apos;http://proxy.test.com:8080&apos;&#125; # 其中proxy.test.com即为代理服务器的地址url = &apos;https://www.baidu.com&apos; # 这个url为要访问的urlresp = requests.get(url,proxies = proxies) 更多详细的用法还是需要阅读官方库，requests几乎可以代替urllib的使用 2.urllib2 使用示例 使用urllib2加载一个网页的html（方便使用Soup进行解析） 123import urllib2response = urllib2.urlopen(&apos;http://python.org/)html = response.read()","categories":[],"tags":[]},{"title":"搭建博客感想","slug":"testArticle","date":"2017-11-22T12:18:56.000Z","updated":"2017-11-22T12:57:25.000Z","comments":true,"path":"2017/11/22/testArticle/","link":"","permalink":"http://Metatronxl.github.io/2017/11/22/testArticle/","excerpt":"","text":"最早是已去中科大数据搬砖的薇薇跟我提起说要搭建博客的事，当时也说好要心血来潮自己彻彻底底的撸完一整套blog代码，flask和django也都学了，暑假也试过自己搭建一个清爽的blog，但是一想到自己还要买域名，买服务器。Emmm…用Github搭建博客才是最吼的！🙂 12这就是我的第一篇博客了，虽然水了点，以后可是要堂堂正正发干货的，坚决不 ~~摸鱼~~ ！ 后记：感谢鹅厂大佬Litten提供的yilia主题 在此附上大佬的github地址：https://github.com/litten","categories":[],"tags":[]}]}